\section{Introduction}~\label{sec:introduction}

\note{This is just a collection of notes for now, and most of it might move into discussion.}

Shor's quantum factoring algorithm requires, at its core, a classical reversible function for modular exponentiation. As
a small example, consider the function $f(r) = 11^{r} \mod 15$. Using the Toffoli construction~\cite{Toffoli:1980}, we
can embed this function $f$ into a reversible function $g$, shown below, such that $g(r,0) = (r,f(r))$:
\[\begin{array}{rcll}
g(r,h) &=& (r,h+1) & \mbox{when~$r$~even~and~$h$~even} \\
g(r,h) &=& (r,h-1) & \mbox{when~$r$~even~and~$h$~odd} \\
g(r,h) &=& (r,11-h) & \mbox{when~$r$~odd~and~$4 > h \geq 0$~or~$12 > h \geq 8$} \\
g(r,h) &=& (r,19-h) & \mbox{when~$r$~odd~and~$8 > h \geq 4$~or~$16 > h \geq 12$}
  \end{array}\]
In Shor's algorithm, $r$ is of the form $2^j$ and to satisfy the complexity requirements of the algorithm, it is
necessary to implement the function $g$ in a way that grows \emph{polynomially} with $j$. As explained in standard
accounts of the algorithm (e.g., the Qiskit implementation), such efficient modular exponentiation circuits are not
straightforward and are the bottleneck in Shor’s algorithm.

P~\cite{shorefficient}

something that uses level 2 is better



Tabulating this function, we observe that it is periodic with
$f(0) = 1, f(1) = 11, f(2) = 0, f(3) = 11, f(4) = 0, \cdots$ or more succinctly:
\[\begin{array}{rcll}
f(r) &=& 1 & \mbox{if~$r$~even} \\
f(r) &=& 11 & \mbox{if~$r$~odd}
\end{array}\]

If $h=0$, we get:

\begin{verbatim}
g(r,0) = (r, 1) when r even
g(r,0) = (r, 11) when r odd
\end{verbatim}

Homotopy type theory (HoTT) introduced the concept of \emph{univalent universes
  of types.} We argue that this concept should be considered a benchmark
property of programming languages generalizing the full abstraction property. To
properly explain the idea and introduce our main results in context, we begin by
recalling the general definition of full abstraction and then specialize it to a small
language of type isomorphisms.

\paragraph*{Full Abstraction.} Consider a programming language equipped with an
operational semantics $\mathcal{O}$ and a denotational semantics
$\mathcal{M}$. As is well-established, each of these semantics induces a notion
of equivalence which we call $\simeq_{\mathcal{O}}$ and $\simeq_{\mathcal{M}}$
respectively.

\begin{definition}[Full Abstraction] A denotational semantics $\mathcal{M}$ is
  \emph{fully abstract} with respect to an operational semantics $\mathcal{O}$
  if the induced equivalences $\simeq_{\mathcal{M}}$ and $\simeq_{\mathcal{O}}$
  coincide.
\end{definition}

In many languages, the correspondence between operational and denotational
equivalences does not hold and when it does it is quite complicated to
establish. However, if we restrict our attention to the language of type
isomorphisms between finite types, the situation is much simpler.

\paragraph*{Type Isomorphisms.} Consider a small universe of types built from
the empty type ($\emptyt$) and the unit type ($\onet$), and closed under sums
$(\sumt)$ and products $(\prodt)$. The elements of these types are the expected
values $v$: the unit value, the left and right injections of values, and the
pairing of values. It is well-known that, under the conventional interpretation
of types, this universe forms a commutative semiring (see for example, the Agda
standard library). It is straightforward (see Sec.~\ref{sec:pi} for details) to
provide a syntax these type isomorphisms as a inductive collection of
combinators $c$ and an operational semantics $\Downarrow$ that reduces an
application $c(v)$ to an answer $v'$. This semantics induces the following
operational equivalence relation.

\begin{definition}[Operational Equivalence]
  Two combinators $c_1$ and $c_2$ are operationally equivalent
  $c_1 \simeq_{\mathcal{O}} c_2$, if for all values $v$, we have
  $c_1(v) \Downarrow v'$ iff $c_2(v) \Downarrow v'$.
\end{definition}

Independently of this operational interpretation of type isomorphisms, a natural
denotational model is to interpret each type $A$ as a natural number $|A|$ and
interpret each combinator $c$ as a permutation $\llbracket c \rrbracket$. This
model induces the following denotational equivalence.

\begin{definition}[Denotational Equivalence]
  Two combinators $c_1$ and $c_2$ are denotationally equivalent
  $c_1 \simeq_{\mathcal{M}} c_2$, if the denoted permutations are equal:
  $\llbracket c_1 \rrbracket = \llbracket c_2 \rrbracket$.
\end{definition}

The full abstraction property for this language follows from a well-established
result by Fiore, Di Cosmo, and Balat~\cite{fioreRemarksIsomorphismsTyped2002}.

\paragraph*{Function Extensionality.} The previous (standard) definitions of
operational and denotational equivalences rely on function extensionality as
they express the equivalence of two combinators $c_1$ and $c_2$ by reasoning
about their behavior at every argument. Generally speaking, extensional equality
can be seen as a logical principle that is invoked when we lack a way to prove
equality between two indistinguishable objects.

add variables to stand for the ``all v'' and try to prove equivalences: ex
morphims from (x+y) to (y+x). Id not there

Denotational: build meaning of a phrase by composing meanings of subphrases (set theory)

Operational build meaning of a phrase by reasoning about interactions with
environment (category theory)

categorial semantics can give you both

quantum: observational equiv says |s1> is the same as |s2>
do we have a denotational way (inside Hilbert space) to show that these two
states are the same?

\[\begin{array}{rcl}
    A \sumt B &\isot& B \sumt A \\
    A \prodt B &\isot& B \prodt A \\
    A \sumt \zerot &\isot& A \\
    A \prodt \zerot &\isot& \zerot \\
    A \prodt \onet &\isot& A \\
    A \prodt (B \prodt C) &\isot& (A \prodt B) \prodt C \\
    A \prodt (B \sumt C) &\isot& (A \prodt B) \sumt (A \prodt C)
  \end{array}\]

something about level 2 term rewriting


No variables in pi so statement much simpler

Fiore

some existing work on a language with just bool; this extends it to a more
realistic language


An operational semantics typically formalizes
program execution as a partial function $e \Downarrow v$ that maps programs $e$
to observable values $v$. This partial function induces a notion of operational
equivalence defined as follows.



An operational semantics typically formalizes
program execution as a partial function $e \Downarrow v$ that maps programs $e$
to observable values $v$. This partial function induces a notion of operational
equivalence defined as follows.



Programs of type $A \rightarrow B$ where $A \isot B$ can be given both an
operational semantics and a denotational semantics.

%% show three programs of type 1 + 1
%% id, id . id, and swap

We may choose to define this notion of ``same'' operationally or
denotationally.


\begin{definition}[Denotational Equivalence]
  Two expressions $e_1$ and $e_2$ are denotationally equivalent
  $e_1 \simeq_{\mathcal{M}} e_2$, if they denote the same element in the
  mathematical domain, i.e., if $\mathcal{M}(e_1) = \mathcal{M}(e_2)$.
\end{definition}

For


give meaning to programs, based on their execution.

The standard operational semantics of In addition to an operational understanding of type isomorphisms

If we interpret a type $A$ as denoting the natural number $\| A \|$, then a
program $p : A \rightarrow B$ where $A \isot B$ denotes a permutation on
$\| A \| (= \| B \|)$ elements. Full abstraction states that the operational
interpretation of type isomorphisms precisely matches the denotational one,
i.e.,
\[
XX
\]

As a tool for investigating program properties, full abstraction can be seen as
a completeness property of denotational semantics: every equivalence of programs
that can be proved operationally, can also be proved by denotational
means. Equivalently, a denotational proof that two terms are not equivalent will
be enough to show that they are not interchangeable in every program context.

There are at least two distinct ways to reason about such type isomorphisms: (i)
operationally in a programming language by writing programs whose types match
the isomorphisms, and denotationally by interpreting the types as mathematical
objects (e.g. sets).

In a programming language, there will be infinite ways of
expressing the isomorphism as a program. Denotationally, for the special case
under consideration, each type could be interpreted as a natural number and each
isomorphism as a permutation.


As an illustration, here is the Agda proof of the first isomorphism above:

%% ⊎-comm : ∀ {a b} (A : Set a) (B : Set b) → (A ⊎ B) ↔ (B ⊎ A)
%% ⊎-comm _ _ = inverse swap swap swap-involutive swap-involutive


% \begin{figure}[t]
% \[
% \begin{array}{rrcll}
% \idc :& t & \iso & t &: \idc \\
% \\
% \identlp :&  0 \sumtype t & \iso & t &: \identrp \\
% \swapp :&  t_1 \sumtype t_2 & \iso & t_2 \sumtype t_1 &: \swapp \\
% \assoclp :&  t_1 \sumtype (t_2 \sumtype t_3) & \iso & (t_1 \sumtype t_2) \sumtype t_3 &: \assocrp \\
% \\
% \identlt :&  1 {\prodtype} t & \iso & t &: \identrt \\
% \swapt :&  t_1 {\prodtype} t_2 & \iso & t_2 {\prodtype} t_1 &: \swapt \\
% \assoclt :&  t_1 {\prodtype} (t_2 {\prodtype} t_3) & \iso & (t_1 {\prodtype} t_2) {\prodtype} t_3 &: \assocrt \\
% \\
% \absorbr :&~ 0 {\prodtype} t & \iso & 0 ~ &: \factorzl \\
% \dist :&~ (t_1 \sumtype t_2) {\prodtype} t_3 & \iso & (t_1 {\prodtype} t_3) \sumtype (t_2 {\prodtype} t_3)~ &: \factor
% \end{array}
% \]
% \caption{$\Pi$-terms.}
% \label{pi-terms}
% \end{figure}

% We now have in our hands our desired denotational semantics for types.
% We want to create a programming language, which we call $\Pi$, such
% that the types and type combinators map to $\bot, \top, \presumtype,
% \preprodtype$, and such that we have ground terms whose denotation are
% all $16$ type isomorphisms of Fig.~\ref{type-isos}. This is rather
% straightforward, as we can simply do this literally. To make the
% analogy with commutative semirings stand out even more, we will use
% $0, 1, \sumtype$, and ${\prodtype}$ at the type level, and will denote
% ``equivalence'' by $\iso$.  Thus Fig.~\ref{pi-terms} shows the
% ``constants'' of the language.  As these all come in symmetric pairs
% (some of which are self-symmetric), we give names for both directions.
% Note how we have continued with the spirit of Curry-Howard: the terms
% of $\Pi$ are \emph{proof terms}, but rather than being witnesses of
% inhabitation, they are witnesses of equivalences. Thus we get an
% unexpected programming language design:

% \begin{center}
% \fbox{ The proof terms denoting commutative semiring equivalences
%   induce the terms of $\Pi$.}
% \end{center}
% \vspace*{3mm}

% \begin{figure}[t]
% \[
% \Rule{}
% {\jdg{}{}{c_1 : t_1 \iso t_2} \quad \vdash c_2 : t_2 \iso t_3}
% {\jdg{}{}{c_1 \odot c_2 : t_1 \iso t_3}}
% {}
% \qquad
% \Rule{}
% {\jdg{}{}{c_1 : t_1 \iso t_2} \quad \vdash c_2 : t_3 \iso t_4}
% {\jdg{}{}{c_1 \oplus c_2 : t_1 \sumtype t_3 \iso t_2 \sumtype t_4}}
% {}
% \]
% \[
% \Rule{}
% {\jdg{}{}{c_1 : t_1 \iso t_2} \quad \vdash c_2 : t_3 \iso t_4}
% {\jdg{}{}{c_1 \otimes c_2 : t_1 {\prodtype} t_3 \iso t_2 {\prodtype} t_4}}
% {}
% \]
% \caption{$\Pi$-combinators.}
% \label{pi-combinators}
% \end{figure}

% \noindent
% Of course, one does not get a programming language with just typed
% constants! There is a need to perform multiple equivalences. There are
% in fact three ways to do this: sequential composition $\odot$, choice
% composition $\oplus$ (sometimes called juxtaposition), and parallel
% composition $\otimes$. See Fig.~\ref{pi-combinators} for the
% signatures. The construction $c_1 \odot c_2$ corresponds to performing
% $c_1$ first, then $c_2$, and is the usual notion of composition -- and
% corresponds to $\fatsemi$ of the language of permutations of
% Sec.~\ref{sec:dataone}. The construction $c_1 \oplus c_2$ chooses to
% perform $c_1$ or $c_2$ depending on whether the input is labelled
% $\textsf{left}$ or $\textsf{right}$ respectively. Finally the
% construction $c_1 \otimes c_2$ operates on a product structure, and
% applies $c_1$ to the first component and $c_2$ to the second. The
% language of permutations lacked the ability to combine permutations by
% taking sums and products, which led to the awkward non-compositional
% programming style illustrated in the full adder
% example~(Eq.~\ref{eq:adder}).


% Thus the denotation of the $\Pi$ terms \emph{should} be
% permutations. But given types $A$ and $B$ denoting $\fin{m}$
% and~$\fin{n}$ respectively, what are $A \presumtype B$ and $A \preprodtype B$ ?
% They correspond exactly to $\fin{m+n}$ and $\fin{m*n}$!
% Geometrically, this corresponds to concatenation for $A + B$,
% i.e. lining up the elements of $A$ first, and then those of~$B$. For
% $A * B$, one can picture this as lining up the elements of $A$
% horizontally, those of $B$ vertically and perpendicular to those of
% $A$, and filling in the square with pairs of elements from $A$ and
% $B$; if one re-numbers these sequentially, reading row-wise, this
% gives an enumeration of $\fin{m*n}$.

% From here, it is easy to see what, for example, $c_1 \oplus c_2$ must be,
% operationally: from a permutation on $\fin{m}$ and another on $\fin{n}$,
% create a permutation on $\fin{m+n}$ by having $c_1$ operate on the first
% $m$ elements of $A+B$, and $c_2$ operate on the last $n$ elements.
% Similarly, $\swapp$ switches the roles of $A$ and $B$, and thus corresponds
% to $\fin{n+m}$. Note how we ``recover'' the commutativity of
% natural number addition from this type isomorphism. Geometrically, $\swapt$
% is also rather interesting: it corresponds to matrix transpose!
% Furthermore, in this representations, some combinators like
% $\identlp$ and $\assoclp$ are identity operations: the underlying representations
% are not merely isomorphic, they are definitionally equal.
% In other words, the passage to $\Nat$ erases some structural information.

% \begin{figure}[t]
% \[
% \Rule{}
% {\jdg{}{}{c_1 : t_1 \iso t_2}}
% {\jdg{}{}{\ !\ c_1 : t_2 \iso t_1}}
% {}
% \]
% \caption{Derived $\Pi$-combinator.}
% \label{derived-pi-combinator}
% \end{figure}

% Embedded in our definition of $\Pi$ is a conscious design decision: to make the
% terms of $\Pi$ \emph{syntactically} reversible. In other words, to
% every $\Pi$ constant, there is another $\Pi$ constant which is its
% inverse. As this is used frequently, we give it the short name $!$,
% and its type is given in Fig.~\ref{derived-pi-combinator}. This
% combinator is \emph{defined}, by pattern matching on the syntax of
% its argument and structural recursion.

% This is not the only choice.  Another would be to add a
% $\mathit{flip}$ combinator to the language; we could then remove
% quite a few combinators as redundant. The drawback is that many
% programs in $\Pi$ become longer. Furthermore, some of the symmetry
% at ``higher levels'' (see next section) is also lost. Since the
% extra burden of language definition and of proofs is quite low, we
% prefer the structural symmetry over a minimalistic language definition.

% \begin{figure}[t]
% \[
% \begin{array}{rrcll}
% \identlsp :&  t \sumtype 0 & \iso & t &: \identrsp \\
% \identlst :&  t {\prodtype} 1 & \iso & t &: \identrst \\
% \\
% \absorbl :&~ t {\prodtype} 0 & \iso & 0 ~ &: \factorzr \\
% \distl :&~ t_1 {\prodtype} (t_2 \sumtype t_3) & \iso & (t_1 {\prodtype} t_2) \sumtype (t_1 {\prodtype} t_3)~ &: \factorl
% \end{array}
% \]
% \caption{Additional $\Pi$-terms.}
% \label{more-pi}
% \end{figure}

% We also make a second design decision, which is to make the $\Pi$
% language itself symmetric in another sense: we want both left
% and right introduction/elimination rules for units, $0$ absorption
% and distributivity. Specifically, we add the $\Pi$-terms of
% Fig.~\ref{more-pi} to our language. These are redundant because
% of $\swapp$ and $\swapt$, but will later enable shorter programs
% and more elegant presentation of program transformations.

This set of isomorphisms is known to be sound and
complete~\cite*{fioreIsomorphismsGenericRecursive2004,fioreRemarksIsomorphismsTyped2002} for isomorphisms of finite
types. Furthermore, it is also universal for hardware combinational circuits~\cite{jamesInformationEffects2012}.



\paragraph*{Type Isomorphisms.} For concreteness consider a small universe of
types consisting of the empty type ($\emptyt$) and the unit type ($\onet$), and
closed under sums $(\sumt)$ and products $(\prodt)$. Under the conventional
interpretation of types, this universe would include familiar type isomorphisms
such as:
\[\begin{array}{rcl}
    A \sumt \zerot &\isot& A \\
    A \prodt \zerot &\isot& \zerot \\
    A \prodt \onet &\isot& A \\
    A \prodt B &\isot& B \prodt A \\
    A \prodt (B \prodt C) &\isot& (A \prodt B) \prodt C \\
    A \prodt (B \sumt C) &\isot& (A \prodt B) \sumt (A \prodt C)
\end{array}\]

There are at least two distinct ways to reason about such type isomorphisms: (i)
operationally in a programming language by writing programs whose types match
the isomorphisms, and denotationally by interpreting the types as mathematical
objects (e.g. sets). In a programming language, there will be infinite ways of
expressing the isomorphism as a program. Denotationally, for the special case
under consideration, each type could be interpreted as a natural number and each
isomorphism as a permutation.

%% code from T.lagda



Types can be isomorphic for example (A * B) and (B * A) but
there are many ways of writing a program witnessing that isomorphism:

f1 (a,b) = (b,a)

f2 (a,b) = (a, (b,())) = ((b,()),a) = (b,((),a)) = (b,a)

AxB => Ax(Bx1) => ((Bx1)xA) => (Bx(1xA)) => (BxA)

etc.

or (1+(1+1)) iso to or (1+(1+1)) in 6 different ways semantically and infinitely
many ways as programs

Even more extreme: 0 is isomorphic to 0; there is only one semantic map id but
many many programs

0 => 0

0 => 0xA => 0x(A+0) => (0xA) + 0x0 => 0x0 + 0xA => 0 + 0xA => 0 + 0 => 0

important to have programs because they are intentional; they display different
tradeoffs: space-time; etc

--

For simple types we would also have a simple set-theoretic way of checking
whether two types are isomorphic: check that their sizes are equal! Then any
program that preserves the sizes is a semantic permutation: for types of size n,
we have n! permutations but many many programs. But is it the case that for
every two types of the same size, we have a program that identifies the two
types.  Full abstraction would give us that for all programs P : A iso B we have
that A is equivalent to B. i.e. any program between types of the same size
defines a permutation and for every permutation there is a program that
implements it. For finite types, that's not too hard.

%%

However that story relies on extensional equality. Consider P1 and P2 that
define the same permutation; their equivalence is via the model. Can we have a
set of rewrite rules that we can use to rewrite P1 to P2... get into the
importance of level 2... can we also have different tradeoffs: how do we prove
P1 is the same as P2: we may have a direct rewrite or a very complicated proof

%%

univalent universe gives you all that in one go: summarize result
level 0
level 1
level 2

establish the result for small language (finite types) and focus on reversible
core because that's the relevant part of the result.

how result connects with other things in category theory, etc.

foundation that can be used for other programming languages with different
types, constructs, etc.

%%

Semantically, obvious that only permutation on empty set is id.

Syntactically, we have 0, 1, +, *  and an infinite number of programs of type 0
<-> 0. How do we prove that they are all equivalent to id?

Obvious induction does not work because after a sequential composition step, the
intermediate type is no longer syntactically 0.

So we add size indices to types. Separate U and UHat. Now the proof is obvious.

%%

Add full pi to pi+ translation; show injective?

\begin{verbatim}
Combinators in the (List 3) are acting on << 4 >>
so

(1 :: 2 :: 0 :: 1 :: nil) of type List 3

corresponds to the sequence:

swap 1 2 ;
swap 2 3 ;
swap 0 1;
swap 1 2

and swap 1 2 is then
   A + (B + (C + (D + 0)) <—> A + (C + (B + (D + 0))

also, swap 2 3 is
   A + (B + (C + (D + 0))) <—> A + (B + (D + (C + 0)))
\end{verbatim}

To cite:

Earliest reference we know about the free symmetric strict monoidal category (as an example of 2-monads):
Two-dimensional monad theory~\cite{blackwellTwodimensionalMonadTheory1989}.

%%

\paragraph*{Categorical Connections.}

Natural numbers under addition form the free commutative monoid on one generator. Categorifying this, we get the free
symmetric monoidal groupoid on generators. This is the groupoid of finite sets and bijections.

More generally, we can construct the free symmetric monoidal groupoid on a groupoid, by taking the action of the
groupoid of finite sets and bijections. We can construct this in HoTT by using the classifying space of the symmetric
group.

The free symmetric monoidal category on a category is a 2-monad on
Cat~\cite{blackwellTwodimensionalMonadTheory1989,abramskyAbstractScalarsLoops2005,leinsterHigherOperadsHigher2004}.
Other applications are Fock Spaces, Generalised Species, Abstract Syntax.

We establish a Curry-Howard-Lambek correspondence for Pi, with 0, 1, +. Categorically, the syntactic groupoid of Pi is
the free symmetric monoidal groupoid on one generator. The groupoid of finite sets and bijections, with coproducts, is
equivalent to the free symmetric monoidal groupoid on one generator, making Pi fully adequate with respect to this
semantics.

The logic part of this is in superstructural reversible logic, which is the free commutative semirig, since there are no
equations.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% fill-column: 120
%%% End:
