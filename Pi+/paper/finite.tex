\section{The groupoid of finite types}~\label{sec:finite}

In this section, we describe \review{the algebraic structure} of the groupoid of
finite types, and give \review{a computable presentation} for it.

\vc{The groupoid of finite types is the free symmetric monoidal groupoid on one
  generator. This can be presented as an algebraic 2-theory, which is our syntax
  for $\PiHatLang$. Vertical categorification of natural numbers as a free
  commutative monoid. See groupoidification.}

\todo{Check Brent Yorgey's thesis?}

In the previous~\cref{sec:univalent}, we established that paths in $\UFin$ are equivalent to families of automorphisms
of $\Fin{n}$ for every $n:\Nat$, that is, bijections on finite sets of size $n$. This is the extensional view of
permutations. In the following sections, we will characterise these permutations syntactically- specifically, using a concept of \textit{a presentation} from the group theory. 

Although the meaning of a permutation - that is, the specific bijective function that it represents - is in some sense all there is, manipulating them syntactically still has its advantages. By writing reversible programs, we think of them in an intensional way. Comparing two programs for equality by evaluating them on all points in the domain is a very crude - and maybe even inefficient - way of doing that. 

It is crude, because there is no way of enforcing additional constraints on the process of transforming one program into another, and no way of inspecting what transformation occured. We can imagine a practical situation in which reversible circuts admit one kind of optimization \jk{FPGA?}, but do not admit another, even though they are equivalent in the extensional sense - or maybe one of these transformations is cheaper than another, or maybe it is important to know which transformation did occur for the \jk{producer(?)} to focus their resources of improving this kind of transformations. By comparing the circuits extensionally, the proof object = the path from one into the other - does not have any structure,  it is just a check that the values match.

On the other hand, even though in general, the equality of circuits requires exponential time jk{triple check}, the proof object can be still very small. This creates structure that can possibly be exploited in a heurisic way. \jk{Additionally, if the goal is to convince third-party that two circuits are the same, we can just present the proof instead of showing the equality directly - for example, by choosing a particularly short one}.

Taking all this into account, it is clear that we must seek a syntactic way of computing the meaning of the circuit, instead of just directly evaluating it to $\Aut[\Fin[n]]$ - since when we go back and quote the permutation as a circuit (in a normal form), there was no "trace" left to see what transformation (what sequence of 2-paths) maps the old one into the new one.

Fortunately, group theory already has language in which such problems are expressed - we will be looking at solving the \textit{word problem}. Because $\PiLang$ is used for reasoning about reversible functions on finite sets, the group that we are interested in is the group of permutations on a fininte set, which is classically know as the symmetric group on $n$ words,
$\Sn$. Putting it in this form allows us to connect to the broader scope of the computational group theory and reuse some of the ideas from there. For example, during the proof, we will use \emph{Lehmer codes} (introduced at \cref{subsec:lehmer}) - a concept used widely in this context, which makes for a very convenient and compact method of representing permutations on a computer.

We begin the section with a brief introduction on doing group theory in HoTT, and state the main results using this language in the subsequent sections.

\todo{Big example: Start from a listed permutation, show Lehmer code, then adjacent swaps.}
\todo{Justification for why we need group theory.}

\subsection{Groups}

From universal algebra, a group is simply a set with a 0-ary constant (the neutral element), a binary operation for group multiplication, and a unary inverse operation. The neutral element has
to satisfy unit and inverse laws, and the multiplication has to be associative.

A very simple example of a group is $\mathbb{Z}$, where the neutral element is 0, the inverse of $k$ is $-k$, and the group multiplication is given by integer addition.

In type theory, a group $G$ can be defined as an $\hSet$ $S$ with the following pieces of data:

\begin{enumerate}
  \item a unit or neutral element $e : S$
  \item a multiplication function $m : S \times S \to S$ written as $(g_{1}, g_{2}) \mapsto g_{1} \mult g_{2}$, that satisfies
        \begin{enumerate}
          \item the unit laws, for all $g : S$, that \( g \mult e \id g \) and \( e \mult g \id g \)
          \item the associativity law, for all $g_{1}, g_{2}, g_{3} : S$, that \( g_{1} \mult (g_{2} \mult g_{3}) \id (g_{1} \mult g_{2}) \mult g_{3} \)
        \end{enumerate}
  \item an inversion function $i : S \to S$ written as $g \mapsto \inv{g}$, that satisfies
        \begin{enumerate}
          \item the inverse laws, for all $g : S$, that \( g \mult \inv{g} \id e \) and \( \inv{g} \mult g \id e \)
        \end{enumerate}
\end{enumerate}

However, more conveniently, in HoTT, we can instead use groupoids to talk about groups. A group can be identified with a
1-object groupoid, using a technique called delooping. The delooping of a group $G$ is a groupoid $\B{G}$ given by a
unique object $\pt$ with self-loops that are 1-paths $\pt \id_{\B{G}} \pt$ corresponding to the elements of $G$. Note
that the group operations are automatically given by operations on the identity type, with $\refl_{\pt}$ for the neutral
element, path composition for the group multiplication, and path inverse for the group inverse. These satisfy the group
laws as well, up to the identity type, using the groupoid coherence laws. Moreover, for 1-groups which are supposed to
be sets, these 2-paths should be propositions, so we have to restrict $\B{G}$ to be a 1-groupoid. Hence, a group is
simply given by a pointed, connected 1-type~\cite*{buchholtzHigherGroupsHomotopy2018,symmetryBook2021}.

For example, given a pointed type $(A:\UU, a:A)$, the automorphism group structure at $a$ is given by $a \id_{A} a$. Of
course, for 1-groups we will require that $a \id_{A} a$ is an $\hSet$, which is enforced by having $A$ be a groupoid. In
our running example for the permutation group on finite sets, we have that $\Fin[n]$ is an $\hSet$, and hence,
$\UFin[n] \defeq \BAut[\Fin[n]]$ is a pointed, connected 1-type, whose loopspace $\loopspace[\BAut[\Fin[n]],F_{n}]$ is
equivalent to $\Aut[\Fin[n]] \defeq (\Fin[n] \eqv \Fin[n]) \eqv (\Fin[n] \id_{\UU} \Fin[n])$, which has the
corresponding automorphism group structure.

\subsection{Free groups}

Given any set $A$, we can naively construct a \emph{free group on $A$},
whose elements are drawn from the alphabet $A$, and closed under the group
operations of multiplication and inverse, identified by the group axioms.
For example, the singleton set generates the additive group of integers $\mathbb{Z}$ we introduced before. In this context, we will call $A$ the generating set of $F(A)$.

Usually, there are are many equations, besides the group axioms, that hold for
the elements of the group. For example, in the group of $\mathbb{Z}/3\mathbb{Z}$, where group multiplication is given by the addition modulo 3, we have an equation $1 + 1 + 1 = 0$, which is not a consequence of group axioms, but rather it is specific to the particular group $\mathbb{Z}/3\mathbb{Z}$. A free group can also be defined by its property that no other equations, except the ones directly implied by the group axioms, such as $g \dot g^[-1] = e$, hold for it. Thus, $\mathbb{Z}/3\mathbb{Z}$ is not a free group.

Working in HoTT, as before, we can again use higher inductive types to define the free group.

\todo{What's the best notation for HITs?}
\begin{definition}
  Given an $\hSet$ $A$, the free group $F(A)$ on it is given by a higher
  inductive type with the following point and path constructors. Notice the
  similarity with the definition of a group structure in~\ref{subsec:groups},
  but note that each operation here is a generator for the type $F(A)$\dots.
  \begin{itemize}
    \item An inclusion function $\eta_{A} : A \to F(A)$
    \item A multiplication function $m : F(A) \times F(A) \to F(A)$
    \item An element $e : F(A)$
    \item An inverse function $i : F(A) \to F(A)$
  \end{itemize}
  \smallskip
  \begin{itemize}
    \item For every $x, y, z : F(A)$, a path $\term{assoc} : m(x, m(y, z)) \id m(m(x, y), z)$
    \item For every $x : F(A)$, paths $\term{unitr} : m(x, e) \id x$ and $\term{unitl} : m(e, x) \id x$
    \item For every $x : F(A)$, paths $\term{invr} : m(x, i(x)) \id e$ and $\term{invl} : m(i(x), x) \id e$
    \item A 0-truncation, for every $x, y : F(A)$ and $p, q : x \id y$, a 2-path $\term{trunc} : p \id q$
  \end{itemize}
\end{definition}

A group homomorphism between groups is a function between the underlying sets
that preserves the group structure, which we write as $G_1 \to^G G_2$. With
this, we can state the universal property of free groups.
\jk{In the HoTT version, which we now use, there is no underlying set}.

\begin{proposition}[Universal Property of $F(A)$]~\label{prop:free-groups}
  Given a group $G$ and a map $f : A \to G$, there is a unique group
  homomorphism $\extend{f} : F(A) \to^G G$ such that $\extend{f} \comp \eta_A
    \htpy f$. Equivalently, composition with $\eta_A$ gives an equivalence $F(A)
    \to^G G \eqv A \to G$. Alternatively, the type of group homomorphisms $h :
    F(A) \to^G G$ satisfying $h \comp \eta_A \htpy f$ is contractible.

  % https://q.uiver.app/?q=WzAsMyxbMCwyLCJBIl0sWzAsMCwiRihBKSJdLFsyLDAsIkciXSxbMCwxLCJcXGV0YV9BIl0sWzAsMiwiZiIsMl0sWzEsMiwiXFxleHRlbmR7Zn0iLDAseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJkYXNoZWQifX19XSxbMyw0LCJcXGlkIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfSwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoibm9uZSJ9LCJoZWFkIjp7Im5hbWUiOiJub25lIn19fV1d
  \[\begin{tikzcd}
      {F(A)} && G \\
      \\
      A
      \arrow[""{name=0, anchor=center, inner sep=0}, "{\eta_A}", from=3-1, to=1-1]
      \arrow[""{name=1, anchor=center, inner sep=0}, "f"', from=3-1, to=1-3]
      \arrow["{\extend{f}}", dashed, from=1-1, to=1-3]
      \arrow["\id", Rightarrow, draw=none, from=0, to=1]
    \end{tikzcd}\]
\end{proposition}

We already mentioned that the structure of a free group $F(A)$ is fully specified by its generating set $A$. Thus, the univerasal property states that if we know how the mapping $f$ acts on the generating set $A$, we also know how does it act on every element of the group $F(A)$.

The HoTT definition of $F(A)$ has lots of path constructors corresponding to
each group axiom. \jk{why did we introduced it? Why is it bad?}. 

Instead, in the proof, we will think about the free group as it was introduced at the beginning: elements of the free group as words over an alphabet of letters from generating set and the set of their formal inverses. If we take the disjoint union of $A$ with itself, that is, $A + A$ as the group's underlying set, we can use $\inl/\inr$ to mark the elements - for example, by stating that $\inl(a)}$ means $a$ and $\inr(a)$ means $\inv{a}$. 
Hence, we can encode the free group using the free monoid - which is to say, lists on $A + A$. Additionally, we need to ensure that group laws hold, so words identified by the group equations are equalt. Thus, elements of the group will actually be equivalence classes of lists on $A + A$.

\begin{definition}~\label{def:presentation}
  Let $A$ be an $\hSet$, and $\List[\blank]$ the free monoid. The free group
  $F(A)$ on $A$ is the set-quotient of $\List[A + A]$ by the congruence closure
  of the relation $a \cons \inv{a} \cons \nil \sim \nil$.
\end{definition}

\begin{proposition}
  $F(A) \defeq \quot{\List[A + A]}{\sim^{\ast}}$ has a group structure, with the
  empty list $\nil$ for the neutral element, multiplication given by list append
  $\append$, and inverse given by list reversal (\jk{THIS IS NOT TRUE}). Further, $F(A)$ with $\eta_A :
    A \to F(A) \defeq \inl(a) \cons \nil$ satisfies the universal property of free
  groups, as stated in the ~\cref{prop:free-groups}.
\end{proposition}

\subsection{Group presentations}

We already talked about the semantic view of groups, but did not yet mention on how to syntactically represent them - which is the role of group presentations.

A presentation of a group builds it by starting from the free group $F(A)$ and introducing a
collection of equations that have to be satisfied in the resulting group. For
example, if we take $F(\unit) \defeq \mathbb{Z}$ and add an equation $1 + 1 + 1
  = 0$, the resulting group would be $\mathbb{Z}/3\mathbb{Z}$. Note that not all groups have finite (or even recursively enumerable) presentations, and that a group can have any number of presentations. \jk{citation}

The generators can be thought of as primitive operations in a (reversible)
programming language, group structure gives the way of composing these
operations and inverting them, and relations describe how these primitive
operations interact with each other.

As we mentioned, instead of opereating directly in the semantics of the group operation, here the
focus is on the syntax. While before, the equality of elements (such as the
result of multiplication of two elements) had to be computed in some external way, now it is
reduced to a \emph{word problem}, i.e., deciding one word - representative of the group elements' equivalence class - can be reduced to another word,
using group's equations.

However, because equations are not directed, is not always possible to construct a
well-behaving rewriting system. In general, the word problem is proven to be undecidable. There exists an algorithm, due to Knuth and Bendix \jk{citation}, that, when succeeds, constructs a well-behaving rewriting system for an arbitrary finite set of (undirected) equations. However, this algorithm produced a system that is often large and hard to reason about \jk{definitely a citation if we can find it}.

\jk{Example?}

\begin{definition}
  Let $A$ be an $\hSet$ and $R : (A + A) \to (A + A) \to \UU$ a binary relation
  on $A + A$. The group $G$ presented by $\langle A ; R \rangle$ is given by the
  set-quotient of the free group $F(A)$ by the closure of $R$, or equivalently,
  as the coequaliser
  \[\begin{tikzcd}
      FR && FA && G
      \arrow[shift right=2, from=1-1, to=1-3]
      \arrow[shift left=2, from=1-1, to=1-3]
      \arrow[two heads, from=1-3, to=1-5]
    \end{tikzcd}\]
\end{definition}

\vc{do we need this level of detail?}
\todo{Universal property}
\todo{Examples: empty relation, van Kampen of $\pi_{1}$}

The above definition is correct because the universal property of the free group allows
for properly extending the relation on the generating set to the whole group.
\jk{Turn that into a propostion?}

\subsection{Presentation of permutation groups}

We have already seen that the group of permutations, symmetric group $\Sn$, can be defined by taking the type $\Aut[\Fin[n]]$, and showing that it has the appropriate
group structure. Alternatively, we talked on how one can define a group by its presentation,defining a set of generators and a set of relations. However, until now, we did not show any specific presentationation for $\Sn$.

In giving a group presentation, there is an element of choice - as we already mentioned, a group
can be presented in many different ways. For example, we could generate the
permutation group on $\Fin[n]$ by using generators that:

\begin{itemize}
  \item swap the $i$-the element with the $(i+1)$-th element, that is, adjacent swaps, or
  \item swap the $i$-th element with the $j$-th element, for arbitrary $i$-s and $j$-s, or
  \item swap the $i$-th element with an element at a fixed position, or
  \item flip a prefix $\Fin[k]$ of $\Fin[n]$ for $k \leq n$, or
  \item cyclically shift any subset of $\Fin[n]$.
\end{itemize}

One way of thinking about these presentations is via sorting algorithms, which
use different primitive operations. A sorting algorithm has to calculate a
permutation of a list or a finite set, which satisfies the invariant of being a
sorted sequence, which means, the primitive operations of a sorting algorithm
should be able to generate all the permutations on a given list. If using a
chosen set of reversible operations, one can write a sorting algorithm, then
those operations generate the permutation group.

For example, bubble sort uses the primitive operation of adjacent swaps,
insertion sort and selection sort use the primitive operation of swapping the
$i$-th element with the $j$-th element, cycle sort uses cyclical shifts of
subsequences, pancake sort uses flips of prefixes of the list, et cetera.
\todo{check!}

The choice of generators for a presentation is important. \todo{Think about this.}

\begin{itemize}
  \item It affects the difficulty of solving the word problem in $\Sn$ and
        formalising the proof of its correctness.
  \item It affects the proof of strong normalisation of reduction for the relations.
  \item It dictates which words are normal forms in this presentation of $\Sn$.
  \item It has to closely match the $\PiLang$ comobinators so that we can prove
        completeness.
  \item It dictates the choice of reversible gates in the synthesis and
        normalisation of boolean circuits which are applications that we show in
        the later sections.
\end{itemize}

Connecting it all back to the language $\PiLang$ we are dealing with, it is possible to construct a simlar, yet simplified language $\PiLangHat$ (introduced at \cref{sec:equivalence}), which operates on adjacent transpositions, but still preservers much of $\PiLang$ structure. Thus, for the proof, a presentation based on adjacent transpositions is used, called \emph{Coxeter presentation}. Not only is this then, in some sense, the natural choice, but it is also a presentation for which we can solve the word problem in a relatively easy, and which is used broadely in such context \jk{by other people? get citations}.

The technical contribution of this section is a proof that the two descriptions of permutation group - $\Aut[\Fin[n]]$ and the Coxeter presentation - are equivalent. By doing that, we bridge the gap between the syntactic and semantic notions in our completeness proof -- by establishing a
correspondence between the meaning of a program (a bijection) and its syntax (a word).

\subsubsection{Coxeter Presentation}

We already mentioned that the Coxter presentation, which is the one will use, is based on adjacent transpositions, meaning that the primitive operations we use are adjacent swaps. When dealing with permutations on an $n + 1$-element set, there are $n$ adjacent transpositions, transposition number $k$ swapping elements $\el{k}$ and $\el{k+1}$. Thus, the generating set would be $\Fin[n]$.

However, $\Sn$ is not a free group, and so we also have to specify the relations of the presentaiton -- the laws that such generators have to satisfy. there are three of them -- it is easiest to visualise them as braid diagrams.

First, swapping the same two elements two times in a row is the same as doing nothing:

\[
  \begin{tabular}{m{0.3\linewidth}m{0.1\linewidth}m{0.3\linewidth}}
    \begin{center}
      \begin{tikzpicture}
        \pic[local bounding box=my braid,braid/.cd,
          number of strands = 2,
          thick]
        {braid={ s_1, s_1}};
      \end{tikzpicture}
    \end{center}
     &
    \(\xlongrightarrow[]{\cancel}\)
     &
    \begin{center}
      \begin{tikzpicture}
        \pic[local bounding box=my braid,braid/.cd,
          number of strands = 2,
          thick]
        {braid={1, 1}};
      \end{tikzpicture}
    \end{center}
  \end{tabular}
\]

Second, when swapping two distinct pairs of elements (i.e. when the indices of two transpositions
differ by at least 1), it does not matter in which order the swapping happens,
that is, we can slide the wires freely.

\[
  \begin{tabular}{m{0.3\linewidth}m{0.1\linewidth}m{0.3\linewidth}}
    \begin{center}
      \begin{tabular}{m{0.3\linewidth}m{0.1\linewidth}m{0.3\linewidth}}
        \begin{tikzpicture}
          \pic[local bounding box=my braid,braid/.cd,
            number of strands = 2,
            thick]
          {braid={ 1, s_1}};
        \end{tikzpicture}
         &
        \(\cdots\)
         &
        \begin{tikzpicture}
          \pic[local bounding box=my braid,braid/.cd,
            number of strands = 2,
            thick]
          {braid={ s_1, 1 }};
        \end{tikzpicture}
      \end{tabular}
    \end{center}
     &
    \(\xlongrightarrow[]{\swap}\)
     &
    \begin{center}
      \begin{tabular}{m{0.3\linewidth}m{0.1\linewidth}m{0.3\linewidth}}
        \begin{tikzpicture}
          \pic[local bounding box=my braid,braid/.cd,
            number of strands = 2,
            thick]
          {braid={ s_1, 1}};
        \end{tikzpicture}
         &
        \(\cdots\)
         &
        \begin{tikzpicture}
          \pic[local bounding box=my braid,braid/.cd,
            number of strands = 2,
            thick]
          {braid={ 1, s_1}};
        \end{tikzpicture}
      \end{tabular}
    \end{center}
  \end{tabular}
\]

The previous two cases were for the transpositions that either completely overlap, or not overlap at all. The third case is thus left -- what happens if we perform transpositions that move the same elements. If the next transposition in the sequence is equal to the the first one, we can write a third law -- braiding relation.

\[
  \begin{tabular}{m{0.3\linewidth}m{0.1\linewidth}m{0.3\linewidth}}
    \begin{center}
      \begin{tikzpicture}
        \pic[local bounding box=my braid,braid/.cd,
          number of strands = 3,
          thick]
        {braid={ s_2, s_1, s_2}};
      \end{tikzpicture}
    \end{center}
     &
    \(\xlongrightarrow[]{\braid}\)
     &
    \begin{center}
      \begin{tikzpicture}
        \pic[local bounding box=my braid,braid/.cd,
          number of strands = 3,
          thick]
        {braid={ s_1, s_2, s_1}};
      \end{tikzpicture}
    \end{center}
  \end{tabular}
\]

\todo{an example/motivation from book.pdf}

This construction is called a Coxeter presentation. Writing it formally, we take the generating set to be $\Fin[n]$, where the element $k$ corresponds to an adjacent transpositions $\tau_k$, which swaps elements $\el{k}$ and $\el{k+1}$. Then, we define a binary relation $\cox$ on the $\List[\Fin[n]]$, which encodes the laws discussed above.

\begin{definition}[$\cox$]
  \begin{align*}
    \cancel
     & : \forall n \to (n \cons n \cons \nil) \cox \nil                                                     \\
    \swap
     & : \forall k, n \to (\suc[k] < n) \to (n \cons k \cons \nil) \cox (k \cons n \cons \nil)              \\
    \braid
     & : \forall n \to (\suc[n] \cons n \cons \suc[n] \cons \nil) \cox (n \cons \suc[n] \cons n \cons \nil) \\
  \end{align*}
\end{definition}

We define $\cox*$ as the congruence closure of $\cox$.

\begin{definition}[$\cox*$]
  \begin{align*}
    \reflr{\cox}
     & : \forall w \to w \cox* w                                                                                                           \\
    \symr{\cox}
     & : \forall w_{1}, w_{2} \to w_{1} \cox* w_{2} \to w_{2} \cox* w_{1}                                                                  \\
    \transr{\cox}
     & : \forall w_{1}, w_{2}, w_{3} \to  w_{1} \cox* w_{2} \to w_{2} \cox* w_{3} \to w_{1} \cox* w_{3}                                    \\
    \congrf{\cox}{\append}
     & : \forall w_{1}, w_{2}, w_{3}, w_{4} \to  w_{1} \cox* w_{2} \to w_{3} \cox* w_{4} \to w_{1} \append w_{3} \cox* w_{2} \append w_{4} \\
    \relr{\cox}
     & : \forall w_{1}, w_{2} \to w_{1} \cox w_{2} \to w_{1} \cox* w_{2}                                                                   \\
  \end{align*}
\end{definition}

\jk{maybe change that?}
Group persentation, as given in the ~\label{def:presentation}, requires the relation to be on the set of $A + A$, where the right copy corresponded to the set of formal inverses of the generators. However, in our case, we can observe that that the constructor $\cancel$ specifies that the inverse of each element is again the same element. Thus, the type $\Sn$ is defined as the set-quotient of $\List[\Fin[n]]$ by $\cox*$.

\begin{definition}[$\Sn$]
  \(\Sn \defeq \quot{\List(\Fin[n])}{\cox*}\)
\end{definition}

The approach to solving the word problem for $\Sn$ we use is to turn the relations $\cox*$ on
$\List[\Fin[n]]$ to a rewriting system $(\List[\Fin[n]],\cox*)$. A normal form of an element can then be computed by following the rewrite rules, and two words can be compared for $\cox*$-equality by comparing their normal forms.

A well-behaved rewriting system has to be strongly normalising and confluent. First, we observe that after throwing out reflexivity and symmetry, the right hand sides of the relations $\cox*$
are strictly smaller than the left hand sides, in terms of the lexicographical ordering on words in $\Fin[n]$. Thus, by directing the relation from left to right, we would get the termination property out of the box.

The system also has to be confluent, meaning that all critical pairs, that is, terms with overlapping possible reduction rules, have to converge. For example, the pairs below converge.

\[
  \begin{array}{lcr}
    \gspan[\braid][\braid]{\tau_2\tau_1\tau_2\tau_1\tau_2}{\tau_1\tau_2\tau_1\tau_1\tau_2}{\tau_2\tau_1\tau_1\tau_2\tau_1}
     &
    \text{or}
     &
    \gspan[\braid][\cancel]{\tau_2\tau_1\tau_2\tau_2}{\tau_1\tau_2\tau_1\tau_2}{\tau_2\tau_1}
  \end{array}
\]

\begin{proof}
\[
    \begin{array}{lcr}
    % https://q.uiver.app/?q=WzAsNixbMiwwLCJcXHRhdV8yXFx0YXVfMVxcdGF1XzJcXHRhdV8xXFx0YXVfMiJdLFswLDEsIlxcdGF1XzFcXHRhdV8yXFx0YXVfMVxcdGF1XzFcXHRhdV8yIl0sWzQsMSwiXFx0YXVfMlxcdGF1XzFcXHRhdV8xXFx0YXVfMlxcdGF1XzEiXSxbMCwyLCJcXHRhdV8xXFx0YXVfMlxcdGF1XzIiXSxbMiwzLCJcXHRhdV8xIl0sWzQsMiwiXFx0YXVfMlxcdGF1XzJcXHRhdV8xIl0sWzAsMSwiYnJhaWQiLDIseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dLFswLDIsImJyYWlkIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkifX19XSxbMSwzLCJjYW5jZWwiLDIseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dLFszLDQsImNhbmNlbCIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6InNxdWlnZ2x5In19fV0sWzIsNSwiY2FuY2VsIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkifX19XSxbNSw0LCJjYW5jZWwiLDAseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dXQ==
    \[\begin{tikzcd}
        && {\tau_2\tau_1\tau_2\tau_1\tau_2} \\
        {\tau_1\tau_2\tau_1\tau_1\tau_2} &&&& {\tau_2\tau_1\tau_1\tau_2\tau_1} \\
        {\tau_1\tau_2\tau_2} &&&& {\tau_2\tau_2\tau_1} \\
        && {\tau_1}
        \arrow["braid"', squiggly, from=1-3, to=2-1]
        \arrow["braid", squiggly, from=1-3, to=2-5]
        \arrow["cancel"', squiggly, from=2-1, to=3-1]
        \arrow["cancel"', squiggly, from=3-1, to=4-3]
        \arrow["cancel", squiggly, from=2-5, to=3-5]
        \arrow["cancel", squiggly, from=3-5, to=4-3]
    \end{tikzcd}\]
    &
    \text{and}
    &
    % https://q.uiver.app/?q=WzAsNSxbMiwwLCJcXHRhdV8yXFx0YXVfMVxcdGF1XzJcXHRhdV8yIl0sWzAsMSwiXFx0YXVfMVxcdGF1XzJcXHRhdV8xXFx0YXVfMiJdLFsyLDQsIlxcdGF1XzJcXHRhdV8xIl0sWzAsMiwiXFx0YXVfMVxcdGF1XzJcXHRhdV8xXFx0YXVfMiJdLFsxLDMsIlxcdGF1XzFcXHRhdV8xXFx0YXVfMlxcdGF1XzEiXSxbMCwxLCJicmFpZCIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6InNxdWlnZ2x5In19fV0sWzAsMiwiY2FuY2VsIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkifX19XSxbMSwzLCJicmFpZCIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6InNxdWlnZ2x5In19fV0sWzMsNCwiYnJhaWQiLDIseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dLFs0LDIsImNhbmNlbCIsMix7InN0eWxlIjp7ImJvZHkiOnsibmFtZSI6InNxdWlnZ2x5In19fV1d
    \[\begin{tikzcd}
        && {\tau_2\tau_1\tau_2\tau_2} \\
        {\tau_1\tau_2\tau_1\tau_2} \\
        {\tau_1\tau_2\tau_1\tau_2} \\
        & {\tau_1\tau_1\tau_2\tau_1} \\
        && {\tau_2\tau_1}
        \arrow["braid"', squiggly, from=1-3, to=2-1]
        \arrow["cancel", squiggly, from=1-3, to=5-3]
        \arrow["braid"', squiggly, from=2-1, to=3-1]
        \arrow["braid"', squiggly, from=3-1, to=4-2]
        \arrow["cancel"', squiggly, from=4-2, to=5-3]
    \end{tikzcd}\]
    \end{array}
\]
\end{proof}

Unfortunately, in the system we defined, this is not true for all critical pairs.
One example is the following, where both endpoints are normal with respect to
$\cox*$ relation.

\[
  \gspan[\braid][\swap]{\tau_3\tau_2\tau_3\tau_1}{\tau_2\tau_3\tau_2\tau_1}{\tau_3\tau_2\tau_1\tau_3}
\]

\subsection{Rewriting via Coxeter}

Because of the counter-example discussed, the relations have to be changed. In this section, we will formally define a rewriting system partially based on the Coxeter relation, prove that it has the desired properties of confluence and strong normalisation.

Fortunately, the change from the rewriting system described before will be small. Thus, we will not lose the connection to the theory of group presentations, where Coxeter relations are a standard notion~\todo{citation} -- to make the result broadly applicable, even though we use the reduction system for computation, we prove that the \jk{relation it gives} is indeed equivalent to the conguence closure of the standard Coxeter relation $\cox$.

The new reduction system $(\List[\Fin[n]], \longcox*)$ has generators corresponding to $\swap$, $\cancel$ and $\braid$. We fix the problem of the non-converging critical pair discussed previously by changing $\braid$ to be a slightly more general relation $\longbraid$.

\todo {but computation is usually done by using Coxeter matrices~\cite{davisGeometryTopologyCoxeter2008}.}

First, we need to define a helper function $n \downf k$.

\begin{definition}[$\downf : (n : \Nat) \to (k : \Nat) \to {\List[\Fin[\suc[k + n]]]}$]
  \begin{align*}
    n \downf \zero   & \defeq \nil                       \\
    n \downf \suc[k] & \defeq (k + n) \cons (n \downf k)
  \end{align*}
\end{definition}

The result of this function is the sequence \(k + n, k + n - 1, k + n - 2, \ldots, n + 1\). Since we think of the number $m$ as the transposition $\tau_m$ which swaps elements $\el{m}$ and $\el{m+1}$, the role of this helper function is to produce a sequence of transpositions -- a permutation -- which moves element $\el{k + n}$ by $k$ places left, shifting all the elements in between one place right. Expressed in terms of the braiding diagram, for
$n = 0$ and $k = 4$, it has the following form:

\[
  \begin{tikzpicture}
    \pic[local bounding box=my braid,braid/.cd,
      number of strands = 5,
      thick]
    {braid={s_4, s_3, s_2, s_1}};
  \end{tikzpicture}
\]

Then, the directed relation $\longcox$ is defined with the following generators.

\begin{definition}[$\longcox$]
  \begin{align*}
    \longcancel
     & : \forall n, l, r \to (l \append n \cons n \cons r) \longcox (l \append r)                                                                   \\
    \longswap
     & : \forall k, n, l, r \to (\suc[k] < n) \to (l \append n \cons k \cons r) \longcox (l \append k \cons n \append r)                            \\
    \longbraid
     & : \forall n, l, r \to (l \append (n \downf 2 + k) \append (1 + k + n) \cons r) \longcox (l \append (k + n) \cons (n \downf 2 + k) \append r) \\
  \end{align*}
\end{definition}

Constructors $\longcancel$ and $\longswap$ correspond directly to the approproiate constructors of $\cox$ and can be visualised in the same way as before. The remaining constructor uses the helper function to exchange the order of a long sequence of transpositions and a single transposition afterwards. For example, for $n = 2$ and $k = 4$, it allows for the reduction
$[7, 6, 5, 4, 3, 7] \longcox* [6, 7, 6, 5, 4, 3]$. Visualized on the braid diagram, it looks as follows:

\[
  \begin{tabular}{m{0.4\linewidth}m{0.1\linewidth}m{0.4\linewidth}}
    \begin{center}
      \begin{tikzpicture}
        \pic[local bounding box=my braid,braid/.cd,
          number of strands = 6,
          thick]
        {braid={s_5, s_4, s_3, s_2, s_1, s_5}};
      \end{tikzpicture}
    \end{center}
     &
    \(\xlongrightarrow[]{\longbraid}\)
     &
    \begin{center}
      \begin{tikzpicture}
        \pic[local bounding box=my braid,braid/.cd,
          number of strands = 6,
          thick]
        {braid={s_4, s_5, s_4, s_3, s_2, s_1}};
      \end{tikzpicture}
    \end{center}
  \end{tabular}
\]

Note that the previous $\braid$ rule is a special case of $\longbraid$, with $k = 0$. 

Like before, we define the $\longcox*$ to be the closure of $\longcox$, but only under reflexivity and transitivity.

\begin{definition}[$\longcox*$]
  \begin{align*}
    \reflr{\longcox}
     & : \forall w \to w \longcox* w                                                                               \\
    \transr{\longcox}
     & : \forall w_{1}, w_{2}, w_{3} \to  w_{1} \longcox w_{2} \to w_{2} \longcox* w_{3} \to w_{1} \longcox* w_{3} \\
  \end{align*}
\end{definition}

Despite the increased complexity of the generators, the rewriting system $(\List[\Fin[n]],\longcox*)$ has the properties
we desire. It satisfies confluence, that is, the Church-Rosser (diamond) property, and it is strongly normalising,
because it produces a unique irreducible normal form. Existence of the normal form is implied by the well-foundedness of
the relation -- informally, it does not contain any infinite decreasing sequences. We follow the terminology
of~\cite{krausCoherenceWellFoundednessTaming2020} to state our results formally.~\vc{check!}

\begin{proposition}
  \leavevmode
  \begin{enumerate}
    \item $\longcox$ is (locally) confluent. For every span $\coxspan{w_{1}}{w_{2}}{w_{3}}$, there is a matching
          extended cospan $\coxcospan*{w_{2}}{w_{3}}{w}$.
    \item $\longcox*$ is confluent. For every extended span $\coxspan*{w_{1}}{w_{2}}{w_{3}}$, there is a matching
          extended cospan $\coxcospan*{w_{2}}{w_{3}}{w}$.
    \item $\longcox*$ is well-founded. With $<$ the lexicographic ordering on $\List[\Fin[n]]$, and $w$ being
          $<$-accessible if every $v < w$ is $<$-accessible, we have that every $w$ is $<$-accesible.
    \item $\longcox*$ is strongly normalising. For every $w$, there exists a unique $v$ such that $w \longcox* v$.
  \end{enumerate}
\end{proposition}

The modified form of the Coxeter relations are unwieldy and difficult to prove properties about by induction, and, as we mentioned, have little connection to the broader scope of computational group theory. However, we can prove that the closures of the two forms of the relations are equivalent, and work up to this equivalence!

\begin{proposition}~\label{prop:coxlongcox}
  $\cox*$ and $\longcox*$ are equivalent in the following sense: for every $w$ and $v$, $w \cox* v$ iff there is a $u$ such that $w \longcox* u$ and $v \longcox* u$.
\end{proposition}
\jk{does the construction above has a name? It is analogous to the one we do in lambda calculus, when taking $\equiv_{\beta}$. We have to write something about this proof using the Church-Rosser idea.}

By strong normalisation, we get a unique choice function $\normf : {\List[\Fin[n]]} \to {\List[\Fin[n]]}$ that produces
a normal form for terms of $\List[\Fin[n]]$. Equivalence classes on $\List[\Fin[n]]$ are precisely those terms that have
the same normal form. We state a few important properties that $\normf$ satisfies.

\begin{proposition}
  \leavevmode
  \begin{enumerate}
    \item For all $l : \List[\Fin[n]]$, we have that $l \cox* \normf(l)$.
    \item $\normf$ is idempotent, that is, $\normf \comp \normf \htpy \normf$.
    \item $\normf$ splits, that is, we have ${\List[\Fin[n]]} \xrightarrow{s} \Sn[n] \xrightarrow{r} {\List[\Fin[n]]}$
          such that $s \comp r \htpy \normf$ and $r \comp s \htpy \idfunc_{\Sn[n]}$.
    \item \(\im{\quotinc} \eqv \Sn \eqv \im{\normf} \).
  \end{enumerate}
\end{proposition}

Note that this relation $\cox*$ is not $\hProp$-valued, that is, $w \cox* v$ is not a proposition because reductions are
not unique. Because of this, the quotient $\Sn$ is not effective, that is, $\quotrel : w \cox* v \to q(w) \id q(v)$ is
not an equivalence. We could truncate the relation to change this, but then the codomain of $\normf$ also needs to be
truncated. Instead, using the $\normf$ function, we could define a new relation
$(w \approx v) \defeq (\normf(w) \id \normf(v))$ which is $\hProp$-valued, and quotient $\List[\Fin[n]]$ by $\approx$.
This is not necessary because our results hold for $\cox*$ using the properties of the rewriting system we proved, and
we do not rely on the effectivity of the quotient.

Finally, we prove the group structure of $\Sn$, and show that it is indeed a group presentation, given by a quotient of
the free group $F(\Fin[n])$.

\begin{proposition}
  There is a group structure on $\Sn$, where the identity element is $\nil$, multiplication is given by list append, and
  inverse is given by list reversal.
\end{proposition}

\begin{proposition}
  $\Sn$ is equivalent to the generated group given by the normal closure of $\cox*$ extended to
  $\List(\Fin[n] + \Fin[n])$ along the codiagonal map $\nabla_{A} : A + A \to A$.
\end{proposition}

\subsection{Lehmer Codes}~\label{subsec:lehmer}

To prove the equivalence between $\Aut[\Fin[n]]$ and $\Sn[n]$, we will need to define functions back and forth between
the two types. The terms in $\Sn$ can be identified with equivalence classes of terms in $\List[\Fin[n]]$ with respect
to the Coxeter relation $\cox*$. The easiest way to define a function out of this presentation is to define it on the
representatives. We know that these are the unique normal forms in the set-quotient given by $\quotinc \comp \normf$,
but now we will describe what these representatives exactly look like, using an encoding called Lehmer
codes~\cite{lehmerTeachingCombinatorialTricks1960}.

\todo{diagram with List (Fin n) being divided into equivalence classes, and Lehmer codes being an image of immersion,
  being the normal form (representative) in each class.}

There are many ways to represent permutations, e.g. inversions, or cycles, or matrices. Lehmer codes are known in
Combinatorial Analysis~\cite{bellmanCombinatorialAnalysis1960} where they are sometimes called "subexcedant sequences",
or "factoriadics", which give the factorial number system. They are a particularly convenient way of representing
permutations on a computer, partly because they are bitwise-optimal: for any $n : \Nat$, the type $\Lehmer[n]$ hass the cardinality $\fac{n}$, and has an easy to consttuct bijection with $\Aut[\Fin[n]]$. .

Formally, we define $\Lehmer[n]$ to be an $n+1$-element tuple, where the position $k \leq n$ stores an element of $\Fin[k]$. Since
the 0-th position is trivial, in practice, it is ignored, and the sequence starts at
1~\cite{duboisTestsProofsCustom2018,vajnovszkiNewEulerMahonian2011}.\todo{There are more examples - listed
  in~\cite{duboisTestsProofsCustom2018}}. We can then define $\Lehmer$ in two equivalent ways, by a simple recursion on
$\Nat$, or as a type family generated by two constructors.

\begin{definition}[$\Lehmer : \Nat \to \UU$]
  \begin{gather*}
    \begin{aligned}
      \Lehmer[\zero]   & \defeq \Fin[\suc[\zero]]                     \\
      \Lehmer[\suc[n]] & \defeq \Fin[\suc[\suc[n]]] \times \Lehmer[n]
    \end{aligned}
    \qquad
    \begin{aligned}
      \lzero & : \Lehmer[\zero]                                                     \\
      \lsuc  & : \forall n, r, (r \leq \suc[n]) \to \Lehmer[n] \to \Lehmer[\suc[n]]
    \end{aligned}
  \end{gather*}
\end{definition}

\todo{Explain why the two definitions are equivalent?}

For a permutation $\sigma : \Aut[\Fin[n]]$, for any element $i: \Fin[n]$, we can define the inversion count of $i$ as the number of elements smaller than it appearing after in the permutation.
\todo{typify}
\begin{definition}
    Given $\sigma : \Aut[\Fin[n]]$, the inversion count of $k: \Fin[n]$ is given by 
    $|\{j < i : \sigma(j) > \sigma(i)\}|$.
\end{definition}

It turns out that knowing just the inversion counts for all the elements, one can reconstruct the starting permutation. Also, observe that the inversion count for element $i$ is guaranteed to be smaller than $i$, thus fitting in the $i$-th place of a Lehmer code tuple. As an example, consider the following tabulated presentation of the permutation:

\todo{fix this figure}

\[
  \sigma =
  \begin{array}{ccccccccccccccc}
    | & 0      & | & 1      & | & 2      & | & 3      & | & 4      & | \\
    \hline                                                             \\
    | & \el{2} & | & \el{1} & | & \el{4} & | & \el{0} & | & \el{3} & | \\
    \hline                                                             \\
  \end{array}
\]

%  0 1 2 3 4
% -----------
% |2|0|1|4|3|
% -----------

The inversion count for $\el{0}$ is 0 (because there are no smaller elements at all), for $\el{1}$ is 0 (because of $\el{0}$ appearing after), for $\el{2}$ is 2 (because of $\el{0}$ and $\el{1}$), for $\el{3}$ is 0 (because it is last in the sequence), and for $\el{4}$ is 1 (because of $\el{3}$). Thus, the Lehmer code for the permutation $\sigma$ is the 5-tuple $l = (0, 1, 2, 0, 1)$.

To construct the presentation of the permutation from the Lehmer code, written using adjacent transpositions, we perform an algorithm similar to \emph{insertion sort}. Starting from the left-most position of the tuple $l$, we'll read the value $v$, insert the new
element at the end of the newly created list, and shift it backward $v$ places.

\begin{center}
  \begin{tabular}{c|p{0.75\linewidth}}
    (0, 0, 2, 0, 1)               & We start from an empty list $[]$                                                                 \\
    (\highlight{{0}}, 0, 2, 0, 1) & We read 0 as the left-most value from $l$. Thus, we append the element $\el{0}$ to our
    list, getting $[\el{0}]$. The element is shifted $0$ places, so it remains in the
    same place.                                                                                                                      \\
    (0, \highlight{{0}}, 2, 0, 1) & Then, similarly, we read another 0 for the element $\el{1}$, append it to the
    list getting $[\el{0}, \el{1}]$, and don't shift it either.                                                                      \\
    (0, 0, \highlight{{2}}, 0, 1) & We read 2 for the next the element $\el{2}$ - we append $\el{2}$ to our list, getting
    $[\el{0}, \el{1}, \el{2}]$, and shift it 2 places right, which results in a list $[\el{0}, \el{2}, \el{1}]$
    \todo{Typeset it nicely, with arrows showing the shifting}.                                                                      \\
    (0, 0, 2, \highlight{{0}}, 1) & Then we read 0 - appending $\el{3}$ and not shifting, getting $[\el{0}, \el{2}, \el{1}, \el{3}]$ \\
    (0, 0, 2, 0, \highlight{{1}}) & Finally, reading 1 for element $\el{4}$ - appending $\el{4}$ to the list and shifting it
    one place right results in the final list $[\el{0}, \el{2}, \el{1}, \el{4}, \el{3}]$                                             \\
  \end{tabular}
\end{center}
\todo{figure}

Writing formally, to turn a code into a permutation written in the Coxeter presentation, we use $\immersion$:

\begin{definition}[$\immersion : (n : \Nat) \to {\Lehmer[n]} \to {\List[\Fin[n]]}$]
  \begin{align*}
    \immersion(\zero, \zero)    & \defeq \nil                                              \\
    \immersion(\suc[n], (r, l)) & \defeq \immersion(n, l) \append ((\suc[n] - r) \downf r)
  \end{align*}
\end{definition}

As described above, the number $r$ at the position $k$ in the tuple describes how many inversions the element $\el{k}$
has. Thus, we need to perform $r$ many adjacent transpositions to get to the desired position. The function that
constructs a sequence of $r$ adjacent transpositions $[\suc[n], \suc[n] - 1, \suc[n] - 2, \ldots \suc[n] - r]$ is
$(\suc[n] - r) \downf r$.

We can show that the function $\immersion$ gives an equivalence betweeen $\Lehmer[n]$ and $\im{\normf}$.

\begin{proposition}
  \leavevmode
  \begin{enumerate}
    \item For any Lehmer code $c$, $\immersion(c)$ is a normal form with resepct to $\cox*$, that is, $\immersion(c)$ is
          in $\im{\normf}$.
    \item Any element of $\im{\normf}$ can be constructed from a unique Lehmer code by $\immersion$, that is, the fibers
          of $\immersion: \Lehmer[n] \to {\im{\normf}}$ are contractible.
  \end{enumerate}
  Therefore, there is an equivalence between $\Lehmer[n]$ and $\im{\normf}$.
\end{proposition}

We can also establish an equivalence between $\im{\normf}$ and $\Sn$, which gives the following.

\begin{proposition}~\label{prop:sn-im-lehmer-equiv}
  For all $n : \Nat$, \( \Sn \eqv \im{\normf} \eqv \Lehmer[n] \).
\end{proposition}

\todo{So now, finally, give an explict example of a normal form, Lehmer immersion}.

\subsection{Running Lehmer codes}

Finally, it is time to complete our goal of characterising the permutation groups. Having produced a Lehmer code by
normalising words in $\Sn$, we need to run it to produce a concrete bijection of finite sets, and, given a bijection
between finite sets, we need to encode it as a Lehmer code. We will prove that these maps construct an equivalence
between the types $\Lehmer[n]$ and $\Aut[\Fin[\suc[n]]]$.

To do so, we need to construct some equivalences by counting the elements of $\Fin[n]$ using its decidable equality.
First, we define a helper type family $\FinExcept{n} : \Fin[n] \to \UU$ which picks out all elements in $\Fin[n]$ except
the one in the argument. Note that $\FinExcept{n}[i]$ for $i : \Fin[n]$ is a subtype of $\Fin[n]$ and is hence an
$\hSet$.

\begin{definition}
  \( \FinExcept{n}[i] \defeq \dsum{j : \Fin[n]}{i \neq j} \).
\end{definition}

We state and prove a few auxiliary lemmas about how $\FinExcept{n}$ interacts with $\Fin$.

\begin{proposition}~\label{prop:fin-finexcept}
  \leavevmode
  \begin{enumerate}
    \item For any $k : \Fin[n]$, $\unit \sqcup \FinExcept{n}[k] \eqv \Fin[n]$.~\label{prop:fin-finexcept-1}
    \item For any $k : \Fin[\suc[n]]$, $\FinExcept{\suc[n]}[k] \eqv \Fin[n]$.~\label{prop:fin-finexcept-2}
    \item For any $n : \Nat$,
          \( \Aut[\Fin[\suc[n]]] \eqv \dsum{k : \Fin[\suc[n]]}{\FinExcept{\suc[n]}[\fzero] \eqv \FinExcept{\suc[n]}[k]} \).~\label{prop:fin-finexcept-3}
  \end{enumerate}
\end{proposition}

\begin{proof}
  The first and second propositions follow from simply constructing the bijections using the decidable equality of
  $\Fin[n]$, and making sure to punch-in and punch-out the element $k$ at the right place.

  The third proposition performs some interesting computations. On the left, we have the type of automorphisms of
  $\Fin[\suc[n]]$. Assume a particular one $\phi : \Fin[\suc[n]] \xrightarrow{\sim} \Fin[\suc[n]]$. To construct an
  element of the pair on the right, we pick the first component to be the image of the element $\zero$ under $\phi$.
  After fixing $k \defeq \phi(\zero)$, we need to compute the bijection in the second component. We remove the two
  elements $\zero$ and $k$ from $\Fin[\suc[n]]$ and construct an equivalence between the resulting types
  $\FinExcept{\suc[n]}[\zero]$ and $\FinExcept{\suc[n]}[k]$ as follows:
  $\FinExcept{\suc[n]}[\zero] \eqv \dsum{k:\Fin[\suc[n]]}{(\phi(\zero) \neq \phi(k))} \eqv \FinExcept{\suc[n]}[k]$.

  To go the other way, given a $k : \Fin[\suc[n]]$ and an equivalence
  $\FinExcept{\suc[n]}[\fzero] \eqv \FinExcept{\suc[n]}[k]$ we compute an equivalence as follows:
  $\Fin[\suc[n]] \eqv \unit \sqcup \FinExcept{\suc[n]}[k] \eqv \unit \sqcup \Fin[n] \eqv \Fin[\suc[n]]$.
\end{proof}

Using these facts, we can now prove the main result of this section.

\begin{proposition}~\label{prop:lehmer-aut-equiv}
  For all $n:\Nat$, \( \Lehmer[n] \eqv \Aut[\Fin[\suc[n]]] \).
\end{proposition}

\begin{proof}
  For $n = \zero$, note that $\Lehmer[\zero]$ is contractible, and so is $\Aut[\Fin[\suc[\zero]]]$. For $n = \suc[m]$,
  we compute a chain of equivalences.
  \begin{gather*}
    \arraycolsep=0.5em\def\arraystretch{1.5}
    \begin{array}{rl}
           & \Lehmer[\zero]          \\
      \eqv & \unit                   \\
      \eqv & \Aut[\Fin[\suc[\zero]]] \\
    \end{array}
    \qquad\qquad
    \begin{array}{rlr}
           & \Lehmer[\suc[m]]                                                                              &                                                                 \\
      \eqv & \Fin[\suc[\suc[m]]] \times \Lehmer[m]                                                         & \text{by definition}                                            \\
      \eqv & \Fin[\suc[\suc[m]]] \times \Aut[\Fin[\suc[m]]]                                                & \text{induction hypothesis}                                     \\
      \eqv & \dsum{k : \Fin[\suc[\suc[m]]]}{\Fin[\suc[m]] \eqv \Fin[\suc[m]]}                              & \text{$\Sigma$ over a constant family}                          \\
      \eqv & \dsum{k : \Fin[\suc[\suc[m]]]}{\FinExcept{\suc[\suc[m]]}[\fzero] \eqv \Fin[\suc[m]]}          & \text{by~\cref{prop:fin-finexcept}.~\ref{prop:fin-finexcept-2}} \\
      \eqv & \dsum{k : \Fin[\suc[\suc[m]]]}{\FinExcept{\suc[\suc[m]]}[\fzero] \eqv \FinExcept{\suc[m]}[k]} & \text{by~\cref{prop:fin-finexcept}.~\ref{prop:fin-finexcept-2}} \\
      \eqv & \Aut[\Fin[\suc[\suc[m]]]]                                                                     & \text{by~\cref{prop:fin-finexcept}.~\ref{prop:fin-finexcept-3}} \\
    \end{array}
  \end{gather*}
\end{proof}

To see how the proof works combinatorically, let us go through an example. We will start from a following bijection $p: \Aut[\Fin[4]]$:
\begin{align*}
    p(0) \mapsto & 1 \\
    p(1) \mapsto & 3 \\
    p(2) \mapsto & 0 \\
    p(3) \mapsto & 2
\end{align*}

At first, we start from a full domain $\Fin[4]$ and full codomain $\Fin[4]$.
\[
    \begin{bmatrix}
        0 \\
        1 \\
        2 \\
        3
    \end{bmatrix}
    \mapsto
    \begin{bmatrix}
        0 \\
        1 \\
        2 \\
        3
    \end{bmatrix}
\]

We remember that the last element of result tuple is $k - 1 = 2$. The appropriate elements on both sides are removed,
forming, respectively, $\FinExcept{4}[0]$ and $\FinExcept{4}[1]$. These are then
both transformed to $\Fin[3]$ using
\cref{prop:fin-finexcept}.~\ref{prop:fin-finexcept-2}, as shown below.

\[
    \begin{bmatrix}
        0 \\
        1 \\
        2
    \end{bmatrix}
    \equiv
    \begin{bmatrix}
        1 \\
        2 \\
        3
    \end{bmatrix}
    \mapsto
    \begin{bmatrix}
        0 \\
        2 \\
        3
    \end{bmatrix}
    \equiv
    \begin{bmatrix}
        0 \\
        1 \\
        2
    \end{bmatrix}
\]

Then, the original permutation specified that the element $\el{1}$ has to be
mapped to $\el{3}$. Since we performed renumbering, now it means that element is
mapped to $\el{2}$ - the resulting tuple now looks like $[2, 1]$. We yet again remove these from the domain and codomain,
forming $\FinExcept{0}[0]$ and $\FinExcept{0}[2]$.

\[
    \begin{bmatrix}
        0 \\
        1
    \end{bmatrix}
    \equiv
    \begin{bmatrix}
        1 \\
        2
    \end{bmatrix}
    \mapsto
    \begin{bmatrix}
        0 \\
        2
    \end{bmatrix}
    \equiv
    \begin{bmatrix}
        0 \\
        1
    \end{bmatrix}
\]

The situation repeats two more times: first, by assinging $\el{0}$ to $\el{1}$,
giving the tuple $[0, 2, 1]$ and transforming the domain and codomain:
\[
    \begin{bmatrix}
        0
    \end{bmatrix}
    \equiv
    \begin{bmatrix}
        1
    \end{bmatrix}
    \mapsto
    \begin{bmatrix}
        0
    \end{bmatrix}
    \equiv
    \begin{bmatrix}
        0
    \end{bmatrix}
\]

And finally, by assinging $\el{0}$ to $\el{0}$, forming a tuple $[0, 0, 2, 1]$ and 

At this point we see that the renumbering of the domain is done in an orderly
way (always shifting by 1), so it can be ignored. On the other hand, the
ordering of the codomain is what matters. Each time we remove an element, all
larger elemens are in effect decreased by one.

To remind ourselves, when we introduced the combinatorial interpretation of
Lehmer codes, the element $\el{r}$ at postition $k$ was said to count the number
of inversions of the element $\el{r}$. Thus, this is exactly the meachanism in
which the induction step works.

This corresponds exactly to what happens in
the \cref{prop:fin-finexcept-3}. Looking at the operation combinatorially, it is
inserting element $\el{r}$ at the position $k$, meaning that it shifts the
element $k$places.\jk{needs rewriting after the example is here}

Viewing the codomain as a vector of four elements, element $\el{0}$ has zero
inverses, element $\el{1}$ has 1 inverse, element $\el{2}$ has 1 inverse, and
element $3$ has 

By composing~\cref{prop:lehmer-aut-equiv} and~\cref{prop:sn-im-lehmer-equiv}, we obtain the final
equivalence.

\begin{corollary}
  For all $n : \Nat$,
  \(
  \Sn \eqv \Lehmer[n] \eqv \Aut[\Fin[\suc[n]]]
  \).
\end{corollary}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% fill-column: 120
%%% End:
